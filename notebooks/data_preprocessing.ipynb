{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99b2425-8120-44f4-b71f-766b07561a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 15:07:07,273 loading file /Users/yitingzhou/.flair/models/ner-english-ontonotes-large/2da6c2cdd76e59113033adf670340bfd820f0301ae2e39204d67ba2dc276cc28.ec1bdb304b6c66111532c3b1fc6e522460ae73f1901848a4d0362cdf9760edb1\n",
      "2022-06-09 15:07:34,255 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.request import urlopen  # b_soup_1.py\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.error import URLError\n",
    "import pandas as pd\n",
    "import requests\n",
    "import socket\n",
    "from urllib.error import URLError, HTTPError\n",
    "from urllib.request import Request, urlopen\n",
    "import os \n",
    "import pickle\n",
    "from newspaper import Article\n",
    "import numpy as np\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "tagger = SequenceTagger.load('ner-ontonotes-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb856ef-7c8d-4c7e-8c83-b388a64bcf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## readin all dataset\n",
    "def read_all_data(name):\n",
    "    all_Data = pd.read_csv(name).drop_duplicates(subset=['source_link'])\n",
    "    all_Data.index.name = 'id'\n",
    "    all_Data = all_Data[all_Data['source_link'].notnull()]\n",
    "    return all_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e445085-d4e5-4ce4-9219-a47950a73e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_text(df):\n",
    "    dates = []\n",
    "    text = []\n",
    "    titles = []\n",
    "    idx_lst = []\n",
    "    keywords = []\n",
    "    summary = []\n",
    "    links = []\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        article = Article(row['source_link'])\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            dates.append(article.publish_date)\n",
    "            text.append(article.text)\n",
    "            titles.append(article.title)\n",
    "            idx_lst.append(idx)\n",
    "            keywords.append(article.keywords)\n",
    "            summary.append(article.summary)\n",
    "            links.append(row['source_link'])\n",
    "        except:\n",
    "            continue\n",
    "    content = {'id': idx_lst, 'links': links, 'dates': dates, 'titles': titles, 'text': text, 'keywords': keywords, 'summary': summary}  \n",
    "    df = pd.DataFrame(content)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d56e66e-e42d-497b-9bd1-2c651d25d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_model(model, df):\n",
    "    loaded_model = pickle.load(open(model, 'rb'))\n",
    "    filter_text = pd.read_csv(df)\n",
    "    filter_text = filter_text[filter_text['titles'].notnull()]\n",
    "    pred_res = loaded_model.predict(list(filter_text['titles']))\n",
    "    filter_text['pred_res'] = pred_res\n",
    "    filter_text = filter_text[filter_text['pred_res'] == 'pos']\n",
    "    return filter_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bed5b9-6f2e-4ca9-a1a8-e711a78aeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nondate(df):\n",
    "    non_dates = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if type(df['dates']) == float:\n",
    "            non_dates.append(row['id'])\n",
    "    return non_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a088c4c7-f75f-4253-afdf-4e55a1831d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_date(df, nond_dates):\n",
    "    datepublish = []\n",
    "    date_dict = {}\n",
    "    for idx, i in enumerate(non_dates):\n",
    "        try:\n",
    "            req = Request(list(df[df['id'] == i]['links'])[0],headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urlopen(req, timeout=10).read()\n",
    "            if 'datepublish' in str(webpage).lower():\n",
    "                text = str(webpage)[str(webpage).lower().index('datepublish'):][:300]\n",
    "                sentence = Sentence(text)\n",
    "                tagger.predict(sentence)\n",
    "                for entity in sentence.get_spans('ner'):\n",
    "                    if entity.tag == 'DATE':\n",
    "                        date_dict[i] = entity.text\n",
    "                        datepublish.append(i)\n",
    "                        break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if type(row['dates']) == float:\n",
    "            if row['id'] in date_dict:\n",
    "                df.loc[idx,'dates'] = date_dict[row['id']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c8e46c-feb7-43c3-8f63-633addafd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainset(df):\n",
    "    GPE = []\n",
    "    LOC = []\n",
    "    DATE = []\n",
    "    TIME = []\n",
    "    INDEX = []\n",
    "    SENTENCE = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if type(row['text']) != float:\n",
    "            for sent in nlp(row['text']).sents:\n",
    "                sub_sent = sent.text.strip()\n",
    "                if sub_sent:\n",
    "                    sentence = Sentence(sub_sent)\n",
    "                    tagger.predict(sentence)\n",
    "                    sub_gpe = []\n",
    "                    sub_date = []\n",
    "                    sub_time = []\n",
    "                    sub_loc = []\n",
    "                    for entity in sentence.get_spans('ner'):\n",
    "                        if entity.tag == 'GPE':\n",
    "                            sub_gpe.append(entity.text)\n",
    "                        if entity.tag == 'DATE':\n",
    "                            sub_date.append(entity.text)\n",
    "                        if entity.tag == 'TIME':\n",
    "                            sub_time.append(entity.text)\n",
    "                        if entity.tag == 'LOC':\n",
    "                            sub_loc.append(entity.text)\n",
    "                    if sub_loc or sub_date or sub_gpe or sub_time:\n",
    "                        GPE.append('|'.join(sub_gpe))\n",
    "                        LOC.append('|'.join(sub_loc))\n",
    "                        DATE.append('|'.join(sub_date))\n",
    "                        TIME.append('|'.join(sub_time))\n",
    "                        INDEX.append(row['id'])\n",
    "                        SENTENCE.append(sub_sent)\n",
    "    res = pd.DataFrame({'id':INDEX, 'text': SENTENCE, 'GPE': GPE, 'LOC':LOC, 'DATE': DATE, 'TIME': TIME})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60dbd3-78c7-4371-9045-81a016f0e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = read_all_data('nasa_global_landslide_catalog_point.csv')\n",
    "filter_by_newspaper_date = get_news_text(original_data)\n",
    "filter_by_model_data = filter_by_model('article_detection.model', filter_by_newspaper_date)\n",
    "non_date_dict = get_nondate(filter_by_model_data)\n",
    "final_filter_date = update_date(filter_by_model_data, non_date_dict)\n",
    "training_date = get_trainset(final_filter_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f2301-4ee2-4d22-82e5-76363f6adc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
